{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3761cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29be27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willy\\AppData\\Local\\Temp/ipykernel_3144/1875190810.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data7 = pd.read_csv('C:/Users/willy/Desktop/V_top_500.csv',header = None,encoding='ISO-8859-1',engine='python',error_bad_lines=False,keep_default_na=False,skip_blank_lines=True)\n",
      "C:\\Users\\willy\\AppData\\Local\\Temp/ipykernel_3144/1875190810.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[2] = data[2]+data[3]+data[4]+data[5]\n"
     ]
    }
   ],
   "source": [
    "#read in datasets and concat datasets\n",
    "\n",
    "data7 = pd.read_csv('C:/Users/willy/Desktop/V_top_500.csv',header = None,encoding='ISO-8859-1',engine='python',error_bad_lines=False,keep_default_na=False,skip_blank_lines=True)\n",
    "data = data7[:45032]\n",
    "data[2] = data[2]+data[3]+data[4]+data[5]\n",
    "df7=data.drop(labels=range(3,46),axis=1)\n",
    "\n",
    "data1 = pd.read_csv('D:/datasets/1.csv',header = None)\n",
    "data2 = pd.read_csv('D:/datasets/2.csv',header = None)\n",
    "data3 = pd.read_csv('D:/datasets/3.csv',header = None)\n",
    "data4 = pd.read_csv('D:/datasets/4.csv',header = None)\n",
    "data5 = pd.read_csv('D:/datasets/5.csv',header = None)\n",
    "data6 = pd.read_csv('D:/datasets/6.csv',header = None)\n",
    "data8 = pd.read_csv('D:/datasets/8.csv',header = None)\n",
    "data9 = pd.read_csv('D:/datasets/9.csv',header = None)\n",
    "data10 = pd.read_csv('D:/datasets/10.csv',header = None)\n",
    "\n",
    "dfs = [df7,data1,data2,data3,data4,data5,data6,data8,data9,data10]\n",
    "df = pd.concat(dfs)\n",
    "df = df.rename(columns={0:'Name',1:'Type',2:'Text'})\n",
    "df = df.drop_duplicates()\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df.index = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ee20342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "import emot\n",
    "emot_obj = emot.core.emot()\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30bfa8",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ed900765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    pack = nltk.pos_tag([word])\n",
    "    tag = pack[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \" \"+UNICODE_EMOJI[emot]+\" \")\n",
    "    return text\n",
    "\n",
    "def replace(new_sentence):\n",
    "    replacement_patterns = [\n",
    "            (r'won\\'t', 'will not'),\n",
    "            (r'can\\'t', 'cannot'),\n",
    "            (r'i\\'m', 'i am'),\n",
    "            (r'ain\\'t', 'is not'),\n",
    "            (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "            (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "            (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "            (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "            (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "            (r'(\\w+)\\'d', '\\g<1> would')]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "    for (pattern, repl) in patterns:\n",
    "        (new_sentence, count) = re.subn(pattern, repl, new_sentence)\n",
    "    return new_sentence\n",
    "\n",
    "def remove_pun(new_sentence):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——+-=“：’；、。，？》《{}'\n",
    "    new_sentence = re.sub(r\"[%s]+\" %punc, \"\",new_sentence)\n",
    "    new_sentence = new_sentence.replace('\\n', ' ').replace('\\r', '').replace('x88¥æª',' ').replace('\\x88é',' ').replace('\\x88',' ').replace('\\x9a',' ').replace('\\x87',' ')\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "def digit_remove(new_sentence):\n",
    "    new_sentence =  \" \".join([word for word in new_sentence.split() if not word.isdigit()])\n",
    "    return new_sentence\n",
    "\n",
    "def stopwords_remove(sentence):\n",
    "    #stoplist = stopwords.words('english')\n",
    "     \n",
    "    with open('C:/Users/willy/Desktop/stopwords.txt') as file:\n",
    "        stoplist = [stopword.replace('\\n', '').lower() for stopword in file.readlines()]\n",
    "    \n",
    "    new_sentence = [word for word in sentence if word not in stoplist]\n",
    "    return new_sentence\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    new_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word) or wordnet.NOUN) for word in sentence]\n",
    "    return new_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c9b33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    \n",
    "    # 1.lowercase\n",
    "    new_sentence = str(sentence).lower()\n",
    "    \n",
    "    # 2.emoji convert\n",
    "    new_sentence = convert_emojis(new_sentence)\n",
    "    \n",
    "    # 3.Abbreviation expansion\n",
    "    replacement_patterns = [\n",
    "        (r'won\\'t', 'will not'),\n",
    "        (r'can\\'t', 'cannot'),\n",
    "        (r'i\\'m', 'i am'),\n",
    "        (r'ain\\'t', 'is not'),\n",
    "        (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "        (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "        (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "        (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "        (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "        (r'(\\w+)\\'d', '\\g<1> would')]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "    for (pattern, repl) in patterns:\n",
    "        (new_sentence, count) = re.subn(pattern, repl, new_sentence)\n",
    "    \n",
    "    #4.Punctuation removal\n",
    "    new_sentence = re.sub(',|!|\\?|\\\"|<|>|\\(|\\)|\\[|\\]|\\{|\\}|@|#|\\+|\\=|\\-|\\_|~|\\&|\\*|\\^|%|\\||\\$|/|`|\\.|\\'',\n",
    "                          '', new_sentence,count=0, flags=0)\n",
    "    #5.digits removal\n",
    "    new_sentence =  \" \".join([word for word in new_sentence.split() if not word.isdigit()])\n",
    "    \n",
    "    #6.Stopwords removal (including 16 types)\n",
    "    with open('C:/Users/willy/Desktop/stopwords.txt') as file:\n",
    "        stoplist = [stopword.replace('\\n', '').lower() for stopword in file.readlines()]\n",
    "    new_sentence = [word for word in new_sentence if word not in stoplist]\n",
    "    \n",
    "    #7.Lemmatization\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    new_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word) or wordnet.NOUN) for word in new_sentence]\n",
    "    \n",
    "\n",
    "    \n",
    "    #8.Sentence tokenization\n",
    "    new_sentence = nltk.word_tokenize(new_sentence)\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4087ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_lower = df['Text'].apply(lambda text: text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db08b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_noemoji = new_sentence_lower.apply(lambda text: convert_emojis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94872aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_replace = new_sentence_noemoji.apply(lambda text: replace(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc53e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_nopun = new_sentence_replace.apply(lambda text: remove_pun(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2b9f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_nodigit = new_sentence_nopun.apply(lambda text: digit_remove(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9bc41f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_tokenize = new_sentence_nodigit.apply(lambda text: nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6695faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_stopwords = new_sentence_tokenize.apply(lambda text: stopwords_remove(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b4a217d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_lemmatize = new_sentence_stopwords.apply(lambda text: lemmatize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "98c1552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [reading, comment, heé, ¥æª, sexual, strength,...\n",
       "1        [edit, not, answer, useless, decide, bring, vi...\n",
       "2                                   [sasuke, limbic, calm]\n",
       "3        [conflict, sasuke, type, frankly, distinguish,...\n",
       "4        [not, understand, vote, sexual, main, motivati...\n",
       "                               ...                        \n",
       "83089    [type, alligns, value, cobra, kai, act, heavil...\n",
       "83090                                      [person, annoy]\n",
       "83091    [argument, not, facet, identify, feeling, peop...\n",
       "83092    [dom, obsess, image, strive, emperor, sacrific...\n",
       "83093    [personally, lot, dutiful, task, protect, fore...\n",
       "Name: Text, Length: 83094, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence_lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1635be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = new_sentence_lemmatize\n",
    "df.index = range(len(df))\n",
    "df.to_csv('preprocess_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f1080b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocess_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c5e469d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(index = df['Text'][df['Text'] == '[]'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d72a0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = range(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "24a44ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('preprocess_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5270f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
